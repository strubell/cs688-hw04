\documentclass[12pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{chngpage}
\usepackage{graphicx}
\usepackage[protrusion=true,expansion,kerning]{microtype}
\usepackage{url}

% adjust margins:
\topmargin=-0.25in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=8.5in
\headsep=0.25in

% document-specific information
\newcommand{\docTitle}{Homework \#4}
\newcommand{\docSubTitle}{}
\newcommand{\docDate}{}
\newcommand{\docClass}{CS688}
\newcommand{\docInstructor}{Marlin}
\newcommand{\authorName}{Emma Strubell}

% header and footer
\pagestyle{fancy}
\lhead{\authorName}
\chead{\docTitle}
\rhead{\docClass\ --\ \docInstructor}   
\lfoot{}
\cfoot{}
\rfoot{\emph{Page\ \thepage\ of\ \pageref{LastPage}}}                          
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\allowdisplaybreaks

\begin{document}
\begin{enumerate}

\item % Question 1
\begin{enumerate}
\item
We can write the conditional probability $P_W(X=x\mid H=h)$ in terms of the joint probability $P_W(X=x,H=h)$:
\begin{align*}
P_W(X=x\mid H=h) &= \frac{P_W(X=x,H=h)}{P_W(H=h)}\\
&= \frac{\exp(-E_W(x,h))}{\sum_{x'}\sum_{h'}\exp(-E_W(x',h'))} \cdot \frac{\sum_{x'}\sum_{h'}\exp(-E_W(x',h'))}{\sum_{x}\exp(-E_W(x,h))}\\
&= \frac{\exp(-E_W(x,h))}{\sum_{x}\exp(-E_W(x,h))}\\
&= \frac{\exp(\sum_d\sum_kW^P_{dk}x_dh_k + \sum_kW^B_kh_k + \sum_dW^C_dx_d)}{\sum_x\exp(\sum_d\sum_kW^P_{dk}x_dh_k + \sum_kW^B_kh_k + \sum_dW^C_dx_d)}\\
&= \frac{\exp(\sum_d\sum_kW^P_{dk}x_dh_k)\exp(\sum_kW^B_kh_k)\exp(\sum_dW^C_dx_d)}{\sum_x\exp(\sum_d\sum_kW^P_{dk}x_dh_k)\exp(\sum_kW^B_kh_k)\exp(\sum_dW^C_dx_d)}\\
&= \frac{\exp(\sum_d\sum_kW^P_{dk}x_dh_k)\exp(\sum_dW^C_dx_d)}{\sum_x\exp(\sum_d\sum_kW^P_{dk}x_dh_k)\exp(\sum_dW^C_dx_d)}\\
&= \frac{\exp(\sum_d\sum_kW^P_{dk}x_dh_k)\exp(\sum_dW^C_dx_d)}{ \exp(0)\exp(0) + \exp(\sum_d\sum_kW^P_{dk}h_k)\exp(\sum_dW^C_d)}\\
&= \frac{\exp(\sum_d\sum_kW^P_{dk}x_dh_k + \sum_dW^C_dx_d)}{1 + \exp(\sum_d\sum_kW^P_{dk}h_k + \sum_dW^C_d)}\\
\end{align*}

For a single $x_d$ this further simplifies to:
\begin{align*}
P_W(X_d=x_d\mid H=h) &= \frac{\exp(\sum_kW^P_{dk}x_dh_k + W^C_dx_d)}{1 + \exp(\sum_kW^P_{dk}h_k + W^C_d)}
\end{align*}

\item
First we take the derivative of the average log likelihood $\mathcal{L}$ with respect to the pairwise parameters $W^P_{ij}$:
\begin{align*}
\frac{\partial\mathcal{L}(W\mid x_{1:N})}{\partial W^P_{ij}} = \frac{1}{N}\sum_{n=1}^N \frac{1}{P_W(X=x_n)}\frac{\partial P_W(X=x_n)}{\partial W^P_{ij}}
\end{align*}

Then the derivative of $P_W(X=x_n)$ with respect to the pairwise parameters $W^P_{ij}$ by applying the quotient rule and simplifying:
\begin{align*}
\frac{\partial P_W(X=x_n)}{\partial W^P_{ij}} &= \frac{\sum_{h}\exp\left(-E_W(x_n,h)\right)}{\sum_{x'}\sum_{h'}\exp\left(-E_W(x', h')\right)}\left(\frac{-\partial E_W(x_n,h)}{\partial W^P_{ij}}\right)\\
&- \frac{\sum_{h}\exp\left(-E_W(x_n,h)\right)}{\left(\sum_{x'}\sum_{h'}\exp\left(-E_W(x', h')\right)\right)^2}\sum_{x'}\sum_{h'}\exp\left(-E_W(x', h')\right)\\
&\left(\frac{-\partial E_W(x',h')}{\partial W^P_{ij}}\right)\\
&= \sum_{h}P_W(X=x_n,H=h)\left(\frac{-\partial E_W(x_n,h)}{\partial W^P_{ij}}\right)\\
&- \sum_{h}P_W(X=x_n,H=h)\sum_{x'}\sum_{h'}P_W(X=x',H=h')\left(\frac{-\partial E_W(x',h')}{\partial W^P_{ij}}\right)\\
&= \sum_{h}P_W(X=x_n,H=h)\left(\frac{-\partial E_W(x_n,h)}{\partial W^P_{ij}}\right)\\
&- \sum_{h}P_W(X=x_n,H=h)\sum_{x'}\sum_{h'}\left(\frac{-\partial E_W(x',h')}{\partial W^P_{ij}}\right)
\end{align*}

And finally the derivative of the energy function $E_W(x_n,h)$ with respect to the pairwise parameters $W^P_{ij}$:
\begin{align*}
\frac{\partial E_W(x_n,h)}{\partial W^P_{ij}} &= -\sum_{i=1}^D\sum_{j=1}^Kx_{ni}h_{j}
\end{align*}

Back-substituting the above into the partial derivative of $P_W(X=x_n)$ with respect to $W^P_{ij}$ gives:
\begin{align*}
\frac{\partial P_W(X=x_n)}{\partial W^P_{ij}} &= \sum_{h}P_W(X=x_n,H=h)\sum_{i=1}^D\sum_{j=1}^Kx_{ni}h_{j}\\
&- \sum_{x'}\sum_{h'}\sum_{i=1}^D\sum_{j=1}^Kx'_{i}h'_{j}
\end{align*}

And back-substituting the above the partial derivative of the log likelihood with respect to $W^P_{ij}$ gives:
\begin{align*}
\frac{\partial\mathcal{L}(W\mid x_{1:N})}{\partial W^P_{ij}} &= \frac{1}{N}\sum_{n=1}^N\Bigg[\frac{1}{P_W(X=x_n)}\sum_{h}P_W(X=x_n,H=h)\sum_{i=1}^D\sum_{j=1}^Kx_{ni}h_{j}\\
&- \sum_{x'}\sum_{h'}\sum_{i=1}^D\sum_{j=1}^Kx'_{i}h'_{j}\Bigg]\\
&= \frac{1}{N}\sum_{n=1}^N\Bigg[\sum_h\sum_{i=1}^D\sum_{j=1}^Kx_{ni}h_{j} - \sum_{x'}\sum_{h'}\sum_{i=1}^D\sum_{j=1}^Kx'_{i}h'_{j}\Bigg]
\end{align*}

Knowing that $x$ and $h$ are each binary, we can expand sums over $x$ and $h$ and cancel 0 terms:
\begin{align*}
\frac{\partial\mathcal{L}(W\mid x_{1:N})}{\partial W^P_{ij}} &= \frac{1}{N}\sum_{n=1}^N\Bigg[x_{ni}P_W(h_j = 1 \mid x=x_n) - P_W(X_i = 1, H_j = 1)\Bigg]\\
&= \frac{1}{N}\sum_{n=1}^N\Bigg[x_{ni}P_W(h_j = 1 \mid x=x_n)\Bigg] - P_W(X_i = 1, H_j = 1)
\end{align*}

\end{enumerate}

\item % Question 2
\begin{enumerate}
\item
Every 5th sample of visible units for 500 iterations of sampling starting from a random binary hidden vector and using given models trained with 100 hidden units:
\begin{center}
\includegraphics[scale=0.7]{2a}
\end{center}
The samples look similar to the 1s from the example data, but they are clearly all 1s and not other numbers. They are maybe a little noisier, too (especially early on.) Since we're starting from a single random sample of hidden units, this random sample probably leads to a part of the state space representing 1s. We can see the 1 evolve from the initial more random setting as we perform more iterations of sampling.

\item
The final sample of visible units for each of 100 block Gibbs chains, each initialized to an independent setting of random binary values: 
\begin{center}
\includegraphics[scale=0.7]{2b}
\end{center}
These samples are much more variable than those from a chain with one initialization. This makes sense since we are able to explore more of the state space, ending up in regions that correspond to different numbers. This trained model still seems to have a preference to 1s, though.

\item
Plot of sample energy over 500 iterations for the first five chains from part (b):
\begin{center}
\includegraphics[scale=0.7]{2c}
\end{center}
The energies of the different chains converge to similar values, though the 1s seem to converge to a more similar value than that of the non-1 contained in the first five samples. They also appear to converge very quickly, suggesting that the burn-in and autocorrelation times for the Gibbs sampler applied to this model are short.

\end{enumerate}

\item % Question 3
\begin{enumerate}
\item
Final sample from each of the 100 Gibbs chains after training using 50 iterations, 400 hidden units, 100 batches, step size $\alpha=0.1$, regularization parameter $\lambda=0.0001$:
\begin{center}
\includegraphics[scale=0.7]{q3a}
\end{center}
The samples look very similar to the examples in Figure 1 of the homework handout, but noisier. Compared to the samples produced by the model with 100 hidden units, these samples vary much more; the preference for 1s is lost, likely because the 400 hidden units are able to encode more information about the 10 different digit classes than the 100 hidden variables, representing more of the state space, which we see in these samples.

\item
Receptive field images for all 400 hidden units:
\begin{center}
\includegraphics[scale=0.65]{rf-1}
\end{center}
\begin{center}
\includegraphics[scale=0.65]{rf-2}
\end{center}
\begin{center}
\includegraphics[scale=0.65]{rf-3}
\end{center}
\begin{center}
\includegraphics[scale=0.65]{rf-4}
\end{center}
There is definitely structure encoded in these receptive fields. Many consist of a dot in a certain part of the image, corresponding to an image that has digit in that sector, some are overall darker than others, representing images where the digit takes up more or less of the space, and some represent more complex strucutre, like a line in a certain direction or the curve of a 0 or 8. The receptive fields do not encode specific digits, which makes sense since they are a lower-dimensional latent representation of lower-level properties shared between digits that combine to make up the digits themselves.

\end{enumerate}

\item % Question 4
\begin{enumerate}
\item Running SVMlight to classify the digits using the lower-dimensional embeddings produced by the RBM with 400 hidden units gives an error rate of 3.71\%.
\item Running SVMlight to classify the digits using the raw binary representations gives an error rate of 7.88\%; the lower dimensional embeddings work better as features for classification. The raw binary values are probably much noisier, while the embeddings distill less noisy latent structural properties of the digits.
\end{enumerate}

\end{enumerate}
\end{document}